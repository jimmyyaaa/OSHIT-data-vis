"""
æ•°æ®ç¼“å­˜ç®¡ç†æ¨¡å—
è´Ÿè´£ç®¡ç†åº”ç”¨çº§åˆ«çš„æ•°æ®ç¼“å­˜ï¼ˆå†…å­˜ DataFrame + ç£ç›˜ JSON æŒä¹…åŒ–ï¼‰
"""
import os
import json
import pickle
from datetime import datetime
from typing import Optional, Dict, Any
import pandas as pd


class DataCache:
    """å•ä¾‹æ•°æ®ç¼“å­˜ç±» - å†…å­˜å­˜ DataFrameï¼Œç£ç›˜å­˜ JSON"""
    
    # ç±»å˜é‡
    _instance: Optional['DataCache'] = None
    _data: Optional[Dict[str, pd.DataFrame]] = None  # å†…å­˜å­˜ DataFrame
    _last_update: Optional[datetime] = None
    
    # ç¼“å­˜æ–‡ä»¶è·¯å¾„
    CACHE_DIR = os.path.join(os.path.dirname(__file__), '.cache')
    CACHE_FILE = os.path.join(CACHE_DIR, 'sheet_data.json')
    METADATA_FILE = os.path.join(CACHE_DIR, 'cache_metadata.json')

    def __new__(cls) -> 'DataCache':
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._init_cache_dir()
        return cls._instance

    def _init_cache_dir(self) -> None:
        """åˆå§‹åŒ–ç¼“å­˜ç›®å½•"""
        if not os.path.exists(self.CACHE_DIR):
            os.makedirs(self.CACHE_DIR)
            print(f"âœ… åˆ›å»ºç¼“å­˜ç›®å½•: {self.CACHE_DIR}")

    def _convert_timestamps(self, data: Dict[str, pd.DataFrame]) -> Dict[str, pd.DataFrame]:
        """
        è½¬åŒ–æ‰€æœ‰ DataFrame ä¸­çš„æ—¶é—´æˆ³åˆ—ä¸º datetime ç±»å‹
        åœ¨ç¼“å­˜æ—¶å°±è½¬åŒ–ï¼Œé¿å…è®¡ç®—æ—¶é‡å¤è½¬åŒ–
        
        Args:
            data: åŸå§‹æ•°æ® {sheet_name: DataFrame}
        
        Returns:
            è½¬åŒ–åçš„æ•°æ®
        """
        result = {}
        for sheet_name, df in data.items():
            df_copy = df.copy()
            
            # è½¬åŒ–å¯èƒ½çš„æ—¶é—´æˆ³åˆ—
            timestamp_cols = [
                'Timestamp(UTC+8)',
                'Timestamp',
                'Date',
                'datetime'
            ]
            
            for col in df_copy.columns:
                if col in timestamp_cols or 'time' in col.lower():
                    try:
                        df_copy[col] = pd.to_datetime(df_copy[col])
                    except Exception as e:
                        print(f"âš ï¸ è½¬åŒ– {sheet_name}.{col} å¤±è´¥: {e}")
            
            result[sheet_name] = df_copy
        
        print("âœ… æ—¶é—´æˆ³è½¬åŒ–å®Œæˆ")
        return result

    @property
    def data(self) -> Optional[Dict[str, pd.DataFrame]]:
        """è·å–ç¼“å­˜æ•°æ®ï¼ˆDataFrame æ ¼å¼ï¼‰"""
        return self._data

    @property
    def last_update(self) -> Optional[datetime]:
        """è·å–æœ€åæ›´æ–°æ—¶é—´"""
        return self._last_update

    @property
    def is_cached(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰å†…å­˜ç¼“å­˜æ•°æ®"""
        return self._data is not None

    @property
    def has_disk_cache(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰ç£ç›˜ç¼“å­˜æ–‡ä»¶"""
        return os.path.exists(self.CACHE_FILE)

    def _load_from_disk(self) -> Optional[Dict[str, pd.DataFrame]]:
        """ä»ç£ç›˜è¯»å–ç¼“å­˜æ–‡ä»¶ï¼ˆJSON â†’ DataFrameï¼‰"""
        try:
            if not os.path.exists(self.CACHE_FILE):
                return None
            
            # è¯»å– JSON æ–‡ä»¶
            with open(self.CACHE_FILE, 'r', encoding='utf-8') as f:
                data_dict = json.load(f)
            
            # è½¬æ¢å› DataFrame
            data_df = {}
            for sheet_name, records in data_dict.items():
                data_df[sheet_name] = pd.DataFrame(records)
            
            # è½¬åŒ–æ—¶é—´æˆ³åˆ—ä¸º datetime ç±»å‹
            data_df = self._convert_timestamps(data_df)
            
            # è¯»å–å…ƒæ•°æ®
            if os.path.exists(self.METADATA_FILE):
                with open(self.METADATA_FILE, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                    self._last_update = datetime.fromisoformat(metadata['last_update'])
            
            print(f"âœ… ä»ç£ç›˜åŠ è½½ç¼“å­˜: {self.CACHE_FILE}")
            return data_df
        except Exception as e:
            print(f"âš ï¸ ä»ç£ç›˜åŠ è½½ç¼“å­˜å¤±è´¥: {e}")
            return None

    def _save_to_disk(self, data: Dict[str, pd.DataFrame]) -> bool:
        """å°† DataFrame ç¼“å­˜è½¬ä¸º JSON ä¿å­˜åˆ°ç£ç›˜"""
        try:
            # è½¬æ¢ DataFrame â†’ Dict
            data_dict = {}
            for sheet_name, df in data.items():
                data_dict[sheet_name] = df.to_dict('records')
            
            # ä¿å­˜ JSON æ•°æ®
            with open(self.CACHE_FILE, 'w', encoding='utf-8') as f:
                json.dump(data_dict, f, ensure_ascii=False, indent=2, default=str)
            
            # ä¿å­˜å…ƒæ•°æ®
            metadata = {
                'last_update': datetime.now().isoformat(),
                'cache_size_mb': os.path.getsize(self.CACHE_FILE) / (1024 * 1024)
            }
            with open(self.METADATA_FILE, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… ç¼“å­˜å·²ä¿å­˜åˆ°ç£ç›˜: {self.CACHE_FILE} ({metadata['cache_size_mb']:.2f}MB)")
            return True
        except Exception as e:
            print(f"âŒ ä¿å­˜ç¼“å­˜åˆ°ç£ç›˜å¤±è´¥: {e}")
            return False

    async def load_data(self, force_refresh: bool = False) -> Dict[str, pd.DataFrame]:
        """
        åˆå§‹åŒ–ç³»ç»ŸåŠ è½½çŠ¶æ€
        
        ä¼˜å…ˆçº§ï¼š
        1. force_refresh=True  â†’ å¼ºåˆ¶é‡æ–°åˆå§‹åŒ–
        2. å†…å­˜çŠ¶æ€å·²åˆå§‹åŒ–     â†’ ç›´æ¥è¿”å›
        3. ç£ç›˜å…ƒæ•°æ®å­˜åœ¨       â†’ ä»ç£ç›˜æ¢å¤çŠ¶æ€
        4. éƒ½æ²¡æœ‰              â†’ æ‰§è¡Œåˆå§‹åŒ–
        
        Args:
            force_refresh: æ˜¯å¦å¼ºåˆ¶é‡ç½®çŠ¶æ€
        
        Returns:
            åŠ è½½çš„æ•°æ®å­—å…¸ (å½“å‰ä¸ºç©ºï¼Œä»…ä¿æŒæ¥å£å…¼å®¹)
        """
        # å¼ºåˆ¶åˆ·æ–°ï¼šæ¸…ç©ºå¹¶é‡æ–°åˆå§‹åŒ–
        if force_refresh:
            print("ğŸ”„ å¼ºåˆ¶åˆ·æ–°ï¼šé‡ç½®ç³»ç»ŸåŠ è½½çŠ¶æ€...")
            self._data = None
            self._last_update = None
            return await self._fetch_and_cache()
        
        # æ£€æŸ¥å†…å­˜ç¼“å­˜
        if self.is_cached and self._data is not None:
            print("âš¡ ç³»ç»Ÿå·²åŠ è½½ (å†…å­˜)")
            return self._data
        
        # æ£€æŸ¥ç£ç›˜ç¼“å­˜
        if self.has_disk_cache:
            print("âš¡ ä»ç£ç›˜æ¢å¤åŠ è½½çŠ¶æ€...")
            disk_data = self._load_from_disk()
            if disk_data is not None:
                self._data = disk_data
                return self._data
        
        # éƒ½æ²¡æœ‰ï¼šæ‰§è¡ŒåŠ è½½
        print("ğŸ”„ é¦–æ¬¡å¯åŠ¨ï¼Œæ‰§è¡Œç³»ç»Ÿåˆå§‹åŒ–...")
        return await self._fetch_and_cache()

    async def _fetch_and_cache(self) -> Dict[str, pd.DataFrame]:
        """
        åˆå§‹åŒ–ç³»ç»ŸçŠ¶æ€
        ç”±äºç›®å‰å·²å…¨éƒ¨è¿ç§»è‡³ SQLï¼Œæ­¤å‡½æ•°ä»…ç”¨äºé‡ç½®åŠ è½½çŠ¶æ€
        
        Returns:
            ç©ºæ•°æ®å­—å…¸
        """
        try:
            print("ğŸ”„ åˆå§‹åŒ–ç³»ç»ŸçŠ¶æ€ (SQL æ¨¡å¼)...")
            
            data = {}
            
            # æ›´æ–°å†…å­˜ç¼“å­˜
            self._data = data
            self._last_update = datetime.now()
            
            # ä¿å­˜å…ƒæ•°æ®åˆ°ç£ç›˜
            self._save_to_disk(data)
            
            return data
        except Exception as e:
            raise Exception(f"Failed to initialize data state: {str(e)}")

    def clear_all_cache(self) -> None:
        """æ¸…ç©ºæ‰€æœ‰ç¼“å­˜ï¼ˆå†…å­˜ + ç£ç›˜ï¼‰"""
        # æ¸…ç©ºå†…å­˜
        self._data = None
        self._last_update = None
        
        # åˆ é™¤ç£ç›˜æ–‡ä»¶
        try:
            if os.path.exists(self.CACHE_FILE):
                os.remove(self.CACHE_FILE)
            if os.path.exists(self.METADATA_FILE):
                os.remove(self.METADATA_FILE)
            print("âœ… å·²æ¸…ç©ºæ‰€æœ‰ç¼“å­˜ï¼ˆå†…å­˜ + ç£ç›˜ï¼‰")
        except Exception as e:
            print(f"âš ï¸ æ¸…ç©ºç£ç›˜ç¼“å­˜å¤±è´¥: {e}")

    def get_cache_info(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ä¿¡æ¯"""
        info = {
            'has_memory_cache': self.is_cached,
            'has_disk_cache': self.has_disk_cache,
            'memory_cache_type': 'DataFrame' if self.is_cached else None,
            'last_update': self._last_update.isoformat() if self._last_update else None,
            'disk_cache_path': self.CACHE_FILE if self.has_disk_cache else None,
        }
        
        if self.has_disk_cache:
            try:
                cache_size_mb = os.path.getsize(self.CACHE_FILE) / (1024 * 1024)
                info['disk_cache_size_mb'] = round(cache_size_mb, 2)
            except:
                pass
        
        return info


# å…¨å±€å•ä¾‹å®ä¾‹
data_cache = DataCache()
